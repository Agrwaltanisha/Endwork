{"cells":[{"cell_type":"markdown","metadata":{"id":"HmFXdrKQC2WU"},"source":["### Use the Frozen Lake environment.\n","\n","https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n","#### **Importing required libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":944,"status":"ok","timestamp":1680779007041,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"KvGIxp4eyALi"},"outputs":[],"source":["import sys\n","import gym\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"mJSJF7YHB-v3"},"source":["#### Intializing the environment"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680779007041,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"C8HXEZGtyIQh","outputId":"8fb14a16-60b8-455b-c85e-e06ac5cb1c08"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = gym.make('FrozenLake-v1',is_slippery=True)"]},{"cell_type":"markdown","metadata":{"id":"RraTBWVvCEZo"},"source":["#### Defining a **Helper function to calculate a state-value**"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680779007041,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"QAoTteifyI2c"},"outputs":[],"source":["# Calculate a state-value function\n","def one_step_lookahead(env, state, V, discount):\n","    '''\n","    V: 2-D tensor\n","        Matrix of size nSxnA, each cell represents \n","        a probability of taking actions\n","    '''\n","    \n","    n_actions = env.action_space.n\n","    action_values = np.zeros(shape=n_actions)\n","    for action in range(n_actions):\n","        for prob, next_state, reward, done in env.env.P[state][action]:\n","            action_values[action] += prob * (reward + discount * V[next_state])\n","    ''' \n","    return:\n","        A vector of length nA containing the expected value of each action\n","    '''\n","    return action_values\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QDbW5lK4CrXI"},"source":["#### Defining a function to **implement an optimal policy** for the Frozen Lake environment."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680779007041,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"EheuMWhxyJd0"},"outputs":[],"source":["def policy_eval(policy, env, discount=1.0, theta=1e-9, max_iter=1000):\n","    \"\"\"    \n","    policy: 2-D tensor\n","        Matrix of size nSxnA, each cell represents \n","        a probability of taking actions\n","    \"\"\"\n","    n_states = env.observation_space.n\n","    # number of evaluation iterations\n","    eval_iters = 1\n","    # initialize a value function for each state as zeros\n","    V = np.zeros(shape=n_states)\n","    # repeat until value change is below the threshold\n","    for i in range(int(max_iter)):\n","        # init a change of value function as zero\n","        delta = 0\n","        # iterate through each state\n","        for state in range(n_states):\n","            # init a new value of current state\n","            v = 0\n","            # Try all possible actions which can be taken from this state\n","            for action, action_prob in enumerate(policy[state]):\n","                # evaluate how good each next state will be\n","                for state_prob, next_state, reward, done in env.P[state][action]:\n","                    # calculate the expected value\n","                    v += action_prob * state_prob * (reward + discount * V[next_state])\n","            # claculate the absolute change of value function\n","            delta = max(delta, np.abs(V[state] - v))\n","            # update value function\n","            V[state] = v\n","        eval_iters += 1\n","        \n","        # terminate if value change is insignificant\n","        if delta < theta:\n","            print(f'Policy evaluation terminated after {eval_iters} iterations.\\n')\n","            return V"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9KeF-r8iDcB3"},"source":["### **Create a Policy Iteration function with the following parameters**\n","* policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.\n","* environment: Initialized OpenAI gym environment object\n","* discount_factor: MDP discount factor.\n","* theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n","* max_iterations: Maximum number of iterations "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680779007042,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"IryDdEYmDs8Y"},"outputs":[],"source":["def policy_iteration(env, discount=1.0, max_iter=1000):\n","\n","    n_states = env.observation_space.n\n","    n_actions = env.action_space.n\n","    \n","    # start with random policy = n_states x n_actions / n_actions\n","    policy = np.ones(shape=[n_states, n_actions]) / n_actions\n","\n","    # init counter of evaluated policies\n","    evaluated_policies = 1\n","    \n","    # repeat until convergence or critical number of iterations reached\n","    for i in range(int(max_iter)):\n","        stable_policy = False\n","        # Evaluate current policy\n","        V = policy_eval(policy, env, discount)\n","        # go through each state & try to improve actions that were taken\n","        for state in range(n_states):\n","            curr_action = np.argmax(policy[state])\n","            # look one step ahead and evaluate if curr_action is optimal\n","            # will try every possible action in a curr_state\n","            action_value = one_step_lookahead(env, state, V, discount)\n","            # select best aciton \n","            best_action = np.argmax(action_value)\n","            # if action didn't change\n","            if curr_action != best_action:\n","                stable_policy = True\n","            # Greedy policy update\n","            policy[state] = np.eye(n_actions)[best_action]\n","        evaluated_policies += 1\n","        # if the algo converged & policy is not changing anymore\n","        if stable_policy:\n","            print(f'Found stable policy after {evaluated_policies:,} evaluations.\\n')\n","            return policy, V"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"a6YgpeNsEJhv"},"source":["### **Create a Value Iteration function with the following parameters**\n","* environment: Initialized OpenAI gym environment object\n","* discount_factor: MDP discount factor\n","* theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this numberd.\n","* max_iterations: Maximum number of iterations "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680779007042,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"v2-KjjYmyKSM"},"outputs":[],"source":["# defining Value iteration algorithm to solve MDP.\n","\n","def value_iteration(env, discount=1e-1, theta=1e-9, max_iter=1e4):\n","\n","    # initalized state-value function with zeros for each env state\n","    V = np.zeros(env.observation_space.n)\n","    \n","    for i in range(int(max_iter)):\n","        # early stopping condition\n","        delta = 0\n","        # update each state\n","        for state in range(env.observation_space.n):\n","            # Do a one-step lookahead to calculate state-action values\n","            action_value = one_step_lookahead(env, state, V, discount)\n","            # select best action to perform based on the highest state-action values\n","            best_action_value = np.max(action_value)\n","            # calculate change in value\n","            delta = max(delta, np.abs(V[state] - best_action_value))\n","            # update the value function for current state\n","            V[state] = best_action_value\n","            \n","        # check if we can stop\n","        if delta < theta:\n","            print(f'\\nValue iteration converged at iteration #{i+1:,}')\n","            break\n","    \n","    # create deterministic policy using the optimal value function\n","    policy = np.zeros(shape=[env.observation_space.n, env.action_space.n])\n","    \n","    for state in range(env.observation_space.n):\n","        # one step lookahead to find the best action for this state\n","        action_value = one_step_lookahead(env, state, V, discount)\n","        #select the best action based on the highest state-action value\n","        best_action = np.argmax(action_value)\n","        # update the policy to perform a better action at a current state\n","        policy[state, best_action] = 1.0\n","    \n","    return policy, V"]},{"cell_type":"markdown","metadata":{"id":"i_r4EthIF2NY"},"source":["### Defining a function to implement eacdh episode"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":397,"status":"ok","timestamp":1680779170114,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"HA0k2Qh8yK7R"},"outputs":[],"source":["def play_episodes(env, episodes, policy, max_action=100, render=False):\n","\n","    wins = 0\n","    total_reward, total_action = 0, 0\n","    \n","    for episode in range(episodes):\n","        state = env.reset()\n","        done, max_a = False, 0\n","        while max_a < max_action:\n","            action = np.argmax(policy[state])\n","            next_state, reward, done, _ = env.step(action)\n","            if render:\n","                env.render()\n","            max_a += 1  # increment actions taken\n","            total_reward += reward  # increment reward received\n","            state = next_state  # set current state to next state\n","            # terminate if we're done and increment `wins`\n","            if done:\n","                wins += 1\n","                break\n","        \n","        total_action += max_a\n","\n","    print(f'Total rewards: {total_reward:,}\\tMax action: {max_a:,}')\n","    \n","    avg_reward = total_reward / episodes\n","    avg_action = total_action / episodes\n","    print('')\n","    '''\n","    return: tuple(wins, total_reward, average_reward)\n","        - wins: The total number of wins the agent has\n","        - total_reward: The agent's total accumulated reward\n","        - average_reward: The agent's average reward\n","    '''\n","    return wins, total_reward, avg_reward, avg_action"]},{"cell_type":"markdown","metadata":{"id":"OVN5jRuOFO0X"},"source":["### Implementing the **agent** in the given environment for 1000 episodes"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680779593085,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"oo2MuWYQFnAW"},"outputs":[],"source":["episodes = 1000\n","\n","def agent(env):\n","\n","    rewards = []\n","  \n","    action_mapping = {\n","        0: '\\u2191',  # up\n","        1: '\\u2192',  # right\n","        2: '\\u2193',  # down\n","        3: '\\u2190'   # left\n","    }\n","    \n","    policies = [\n","        ('Policy Iteration', policy_iteration),\n","        ('Value Iteration', value_iteration)\n","    ]\n","    \n","    for iter_name, iter_func in policies:\n","        policy, V = iter_func(env)\n","        \n","        print(f'Final policy using {iter_name}:')\n","        print(' '.join([action_mapping[action] for action in np.argmax(policy, axis=1)]))\n","        \n","        wins, total_reward, avg_reward, avg_action = play_episodes(env, episodes, policy)\n","        rewards.append(total_reward)\n","        \n","        print(f'number of wins = {wins:,}')\n","        print(f'average reward = {avg_reward:.2f}')\n","        print(f'average action = {avg_action:.2f}')\n","\n","    return rewards\n","\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1662,"status":"ok","timestamp":1680779597506,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"g2UG0B9OGdjA","outputId":"16ec0a8c-87c9-4982-ff5b-dbd00a662d66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Policy evaluation terminated after 66 iterations.\n","\n","Found stable policy after 2 evaluations.\n","\n","Final policy using Policy Iteration:\n","↑ ← ↑ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n","Total rewards: 720.0\tMax action: 61\n","\n","number of wins = 1,000\n","average reward = 0.72\n","average action = 41.19\n","\n","Value iteration converged at iteration #8\n","Final policy using Value Iteration:\n","→ ← ↓ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n","Total rewards: 443.0\tMax action: 14\n","\n","number of wins = 1,000\n","average reward = 0.44\n","average action = 28.35\n"]}],"source":["rewards = agent(env)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1680779598832,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"5MWWXWTgGl6P"},"outputs":[],"source":["#env.close()"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1680779599240,"user":{"displayName":"Venkata Ratna","userId":"10796771328777583033"},"user_tz":-330},"id":"PAkTl1D-RPxF","outputId":"c6f60743-d829-4610-a7a4-0691da5e644f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[720.0, 443.0]\n"]}],"source":["print(rewards)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOvGi4pjVDc2EqkFSWTRK8W","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
